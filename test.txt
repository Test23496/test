use std::net::IpAddr;
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::RwLock;
use tokio::time;
use k8s_openapi::api::core::v1 as v1;
use k8s_openapi::api::networking::v1 as networkingapiv1;
use k8s_openapi::apimachinery::pkg::apis::meta::v1 as metav1;
use kube::{
    Api, Client, ResourceExt,
    runtime::{controller::Action, watcher, Controller as KubeController, reflector},
};
use kube::api::{ListParams, Patch, PatchParams, PostParams};
use tracing::{info, warn};

const CONTROLLER_NAME: &str = "kubernetes-service-cidr-controller";
const DEFAULT_SERVICE_CIDR_NAME: &str = "kubernetes";

/// NewController 返回一个新的 *Controller，它从 `--service-cluster-ip-range` 标志生成默认的 ServiceCIDR
/// 并在必要时重新创建它，但如果不同则不更新它。
/// 它遵循与 kubernetes.default Service 相同的逻辑。
pub fn new_controller(
    primary_range: ipnetwork::IpNetwork,
    secondary_range: Option<ipnetwork::IpNetwork>,
    client: Client,
) -> Controller {
    let mut cidrs = vec![primary_range.to_string()];
    if let Some(secondary) = secondary_range {
        cidrs.push(secondary.to_string());
    }

    Controller {
        cidrs,
        client,
        interval: Duration::from_secs(10), // 与 DefaultEndpointReconcilerInterval 相同
        reported_mismatched_cidrs: Arc::new(RwLock::new(false)),
        reported_not_ready_condition: Arc::new(RwLock::new(false)),
    }
}

/// Controller 管理基于选择器的服务 ipAddress。
pub struct Controller {
    cidrs: Vec<String>, // 顺序很重要，第一个 cidr 定义默认 IP 族

    client: Client,

    interval: Duration,
    reported_mismatched_cidrs: Arc<RwLock<bool>>,
    reported_not_ready_condition: Arc<RwLock<bool>>,
}

impl Controller {
    /// Start 在默认 ServiceCIDR 存在或 stopCh 关闭之前不会返回。
    pub async fn start(&self, ctx: tokio::context::Context) {
        info!("Starting {}", CONTROLLER_NAME);

        let api: Api<networkingapiv1::ServiceCIDR> = Api::all(self.client.clone());

        // 等待直到第一次成功同步
        // 这会阻塞 apiserver 启动，所以使用短间隔轮询
        let mut interval = time::interval(Duration::from_millis(100));
        loop {
            tokio::select! {
                _ = ctx.done() => {
                    info!("Shutting down {}", CONTROLLER_NAME);
                    return;
                }
                _ = interval.tick() => {
                    match self.sync().await {
                        Ok(_) => break,
                        Err(e) => {
                            info!("error initializing the default ServiceCIDR: {:?}", e);
                        }
                    }
                }
            }
        }

        // 在后台以定义的间隔运行同步循环
        let mut sync_interval = time::interval(self.interval);
        loop {
            tokio::select! {
                _ = ctx.done() => {
                    info!("Shutting down {}", CONTROLLER_NAME);
                    return;
                }
                _ = sync_interval.tick() => {
                    if let Err(e) = self.sync().await {
                        info!("error trying to sync the default ServiceCIDR: {:?}", e);
                    }
                }
            }
        }
    }

    async fn sync(&self) -> Result<(), kube::Error> {
        let api: Api<networkingapiv1::ServiceCIDR> = Api::all(self.client.clone());

        // 检查默认 ServiceCIDR 是否已存在
        match api.get(DEFAULT_SERVICE_CIDR_NAME).await {
            Ok(service_cidr) => {
                // 单栈到双栈升级
                if self.cidrs.len() == 2
                    && service_cidr.spec.as_ref().map(|s| s.cidrs.as_ref().map(|c| c.len()).unwrap_or(0)).unwrap_or(0) == 1
                    && service_cidr.spec.as_ref().and_then(|s| s.cidrs.as_ref()).map(|c| &c[0]) == Some(&self.cidrs[0])
                {
                    info!(
                        "Updating default ServiceCIDR from single-stack ({:?}) to dual-stack ({:?})",
                        service_cidr.spec.as_ref().and_then(|s| s.cidrs.as_ref()),
                        self.cidrs
                    );
                    let mut updated = service_cidr.clone();
                    if let Some(spec) = updated.spec.as_mut() {
                        spec.cidrs = Some(self.cidrs.clone());
                    }
                    
                    match api.replace(DEFAULT_SERVICE_CIDR_NAME, &PostParams::default(), &updated).await {
                        Err(e) => {
                            info!(
                                "The default ServiceCIDR can not be updated from {} to dual stack {:?} : {:?}",
                                self.cidrs[0], self.cidrs, e
                            );
                            // 事件记录器调用
                        }
                        Ok(_) => {}
                    }
                } else {
                    self.sync_status(&service_cidr).await;
                }
                Ok(())
            }
            Err(kube::Error::Api(err)) if err.code == 404 => {
                // 默认 ServiceCIDR 不存在
                info!("Creating default ServiceCIDR with CIDRs: {:?}", self.cidrs);
                let service_cidr = networkingapiv1::ServiceCIDR {
                    metadata: metav1::ObjectMeta {
                        name: Some(DEFAULT_SERVICE_CIDR_NAME.to_string()),
                        ..Default::default()
                    },
                    spec: Some(networkingapiv1::ServiceCIDRSpec {
                        cidrs: Some(self.cidrs.clone()),
                    }),
                    status: None,
                };

                match api.create(&PostParams::default(), &service_cidr).await {
                    Ok(created) => {
                        self.sync_status(&created).await;
                        Ok(())
                    }
                    Err(kube::Error::Api(err)) if err.code == 409 => {
                        // AlreadyExists
                        Ok(())
                    }
                    Err(e) => {
                        // 事件记录器调用
                        Err(e)
                    }
                }
            }
            Err(e) => Err(e),
        }
    }

    async fn sync_status(&self, service_cidr: &networkingapiv1::ServiceCIDR) {
        // 如果 ServiceCIDR 正在被删除，则不同步其状态，
        // 删除必须由 controller-manager 处理
        if service_cidr.metadata.deletion_timestamp.is_some() {
            tracing::debug!("ServiceCIDR {} is being deleted, skipping status sync", service_cidr.name_any());
            return;
        }

        // 如果 Ready 条件不存在且 CIDR 值与此控制器的 CIDR 值匹配，
        // 此控制器将把 Ready 条件设置为 true。
        let spec_cidrs = service_cidr.spec.as_ref().and_then(|s| s.cidrs.as_ref());
        let same_config = spec_cidrs.map(|c| c == &self.cidrs).unwrap_or(false);
        let current_ready_condition = service_cidr
            .status
            .as_ref()
            .and_then(|s| s.conditions.as_ref())
            .and_then(|conditions| {
                conditions.iter().find(|c| c.type_ == networkingapiv1::SERVICE_CIDR_CONDITION_READY)
            });

        // 处理不一致的配置
        if !same_config {
            let mut reported = self.reported_mismatched_cidrs.write().await;
            if !*reported {
                info!(
                    "Inconsistent ServiceCIDR status for {}, controller configuration: {:?}, ServiceCIDR configuration: {:?}. Configure the flags to match current ServiceCIDR or manually delete it.",
                    service_cidr.name_any(),
                    self.cidrs,
                    spec_cidrs
                );
                // 事件记录器调用
                *reported = true;
            }
            // 无论当前的 Ready 条件如何，不一致的配置都是一个问题。
            // 在这种情况下，我们不尝试更改 Ready 状态。
            return;
        }

        // 配置一致 (sameConfig)
        match current_ready_condition {
            // 当前 Ready=False，不应该发生的状态。
            // 不要尝试覆盖由其他组件设置的 Ready=False 以避免热循环。
            // 默认 ServiceCIDR 永远不应该将此条件设置为 False，如果是这种情况，
            // 那么需要集群管理员的干预。
            Some(condition) if condition.status == "False" => {
                let mut reported = self.reported_not_ready_condition.write().await;
                if !*reported {
                    info!(
                        "Default ServiceCIDR condition Ready is False, but controller configuration matches. Please validate your cluster's network configuration. serviceCIDR: {}, status: {}, reason: {}, message: {}",
                        service_cidr.name_any(),
                        condition.status,
                        condition.reason.as_deref().unwrap_or(""),
                        condition.message.as_deref().unwrap_or("")
                    );
                    // 事件记录器调用
                    *reported = true;
                }
            }

            // 当前 Ready=True 且配置匹配，无需操作。
            Some(condition) if condition.status == "True" => {
                tracing::debug!(
                    "ServiceCIDR {} is Ready and configuration matches. No status update needed.",
                    service_cidr.name_any()
                );
            }

            // 未设置条件且 ServiceCIDR 与此 apiserver 配置匹配，将条件设置为 True
            None | Some(_) => {
                info!("Setting default ServiceCIDR condition Ready to True");
                
                let now = metav1::Time(chrono::Utc::now());
                let condition = metav1::Condition {
                    type_: networkingapiv1::SERVICE_CIDR_CONDITION_READY.to_string(),
                    status: "True".to_string(),
                    observed_generation: service_cidr.metadata.generation,
                    last_transition_time: now,
                    reason: "Ready".to_string(),
                    message: "Kubernetes default Service CIDR is ready".to_string(),
                };

                let mut status = service_cidr.status.clone().unwrap_or_default();
                let mut conditions = status.conditions.unwrap_or_default();
                
                // 更新或添加条件
                if let Some(pos) = conditions.iter().position(|c| c.type_ == networkingapiv1::SERVICE_CIDR_CONDITION_READY) {
                    conditions[pos] = condition;
                } else {
                    conditions.push(condition);
                }
                status.conditions = Some(conditions);

                let api: Api<networkingapiv1::ServiceCIDR> = Api::all(self.client.clone());
                let patch = serde_json::json!({
                    "status": status
                });

                match api
                    .patch_status(
                        DEFAULT_SERVICE_CIDR_NAME,
                        &PatchParams::apply(CONTROLLER_NAME).force(),
                        &Patch::Apply(&patch),
                    )
                    .await
                {
                    Err(e) => {
                        info!("error updating default ServiceCIDR status: {:?}", e);
                        // 事件记录器调用
                    }
                    Ok(_) => {}
                }
            }
        }
    }
}

===============================================

use std::sync::Arc;
use std::time::Duration;
use tokio::sync::RwLock;
use tokio::time::{sleep, Instant};
use k8s_openapi::api::coordination::v1::{Lease, LeaseSpec};
use k8s_openapi::api::coordination::v1beta1::LeaseCandidate;
use k8s_openapi::apimachinery::pkg::apis::meta::v1::{MicroTime, ObjectMeta, Time};
use kube::{
    api::{Api, ListParams, Patch, PatchParams, PostParams},
    client::Client,
    runtime::{
        controller::{Action, Controller as KubeController},
        events::{Event, EventType, Recorder, Reporter},
        finalizer::{finalizer, Event as FinalizerEvent},
        watcher::Config,
        Controller as RuntimeController,
    },
    Resource, ResourceExt,
};
use semver::Version;
use thiserror::Error;
use tracing::{error, info, warn};

const CONTROLLER_NAME: &str = "leader-election-controller";

// Requeue interval is the interval at which a Lease is requeued to verify that it is
// being renewed properly.
const DEFAULT_REQUEUE_INTERVAL: Duration = Duration::from_secs(5);
const NO_REQUEUE: Duration = Duration::from_secs(0);

const DEFAULT_LEASE_DURATION_SECONDS: i32 = 5;

const ELECTION_DURATION: Duration = Duration::from_secs(5);

const LEASE_CANDIDATE_VALID_DURATION: Duration = Duration::from_secs(30 * 60); // 30 minutes

#[derive(Error, Debug)]
pub enum ControllerError {
    #[error("Kubernetes error: {0}")]
    KubeError(#[from] kube::Error),
    
    #[error("No candidates available")]
    NoCandidates,
    
    #[error("No available candidates")]
    NoAvailableCandidates,
    
    #[error("Could not find suitable electee")]
    NoSuitableElectee,
    
    #[error("Candidates at same binary version but received differing strategies: {0}, {1}")]
    ConflictingStrategies(String, String),
    
    #[error("Semver parse error: {0}")]
    SemverError(#[from] semver::Error),
    
    #[error("Error reading lease: {0}")]
    LeaseReadError(String),
}

/// Controller is the leader election controller, which observes component identity leases for
/// components that have self-nominated as candidate leaders for leases and elects leaders
/// for those leases, favoring candidates with higher versions.
pub struct Controller {
    lease_api: Api<Lease>,
    lease_candidate_api: Api<LeaseCandidate>,
    client: Client,
}

impl Controller {
    pub fn new(client: Client, namespace: &str) -> Self {
        Self {
            lease_api: Api::namespaced(client.clone(), namespace),
            lease_candidate_api: Api::namespaced(client.clone(), namespace),
            client,
        }
    }

    pub async fn run(self, workers: usize) -> Result<(), ControllerError> {
        info!("Workers: {}", workers);
        
        // Start workers
        let mut handles = vec![];
        for i in 0..workers {
            info!("Starting worker {}", i);
            let controller = self.clone();
            let handle = tokio::spawn(async move {
                controller.run_election_worker().await;
            });
            handles.push(handle);
        }
        
        // Wait for all workers
        for handle in handles {
            let _ = handle.await;
        }
        
        Ok(())
    }

    async fn run_election_worker(&self) {
        loop {
            if !self.process_next_election_item().await {
                break;
            }
            sleep(Duration::from_secs(1)).await;
        }
    }

    async fn process_next_election_item(&self) -> bool {
        // In a real implementation, this would get items from a work queue
        // For now, we'll just sleep
        sleep(Duration::from_secs(1)).await;
        true
    }

    fn enqueue_candidate(&self, lc: &LeaseCandidate) {
        // Ignore candidates that transitioned to Pending because reelection is already in progress
        if let (Some(ping_time), Some(renew_time)) = (&lc.spec.ping_time, &lc.spec.renew_time) {
            if renew_time.0 < ping_time.0 {
                return;
            }
        }
        // Add to queue (implementation depends on queue structure)
    }

    fn enqueue_lease(&self, lease: &Lease) {
        // Add to queue (implementation depends on queue structure)
    }

    async fn election_needed(
        &self,
        candidates: &[LeaseCandidate],
        lease_name: &str,
        namespace: &str,
    ) -> Result<bool, ControllerError> {
        let lease_result = self.lease_api.get(lease_name).await;
        
        let lease = match lease_result {
            Ok(l) => l,
            Err(kube::Error::Api(err)) if err.code == 404 => {
                return Ok(true);
            }
            Err(e) => {
                return Err(ControllerError::LeaseReadError(e.to_string()));
            }
        };

        if is_lease_expired(&lease) || lease.spec.as_ref()
            .and_then(|s| s.holder_identity.as_ref())
            .map_or(true, |h| h.is_empty())
        {
            return Ok(true);
        }

        // every 15min enforce an election to update all candidates. Every 30min we garbage collect.
        let now = Instant::now();
        for candidate in candidates {
            if let Some(renew_time) = &candidate.spec.renew_time {
                let renew_instant = time_to_instant(&renew_time.0);
                if renew_instant + LEASE_CANDIDATE_VALID_DURATION / 2 < now {
                    return Ok(true);
                }
            }
        }

        let prelim_strategy = pick_best_strategy(candidates)?;
        if prelim_strategy != "OldestEmulationVersion" {
            tracing::debug!("Strategy {:?} is ignored by CLE", prelim_strategy);
            return Ok(false);
        }

        let prelim_electee = pick_best_leader_oldest_emulation_version(candidates);
        if let Some(electee) = prelim_electee {
            if let Some(spec) = &lease.spec {
                if let Some(holder) = &spec.holder_identity {
                    if electee.metadata.name.as_ref() == Some(holder) {
                        tracing::debug!(
                            "Leader {} is already most optimal for lease {}",
                            holder,
                            lease_name
                        );
                        return Ok(false);
                    }
                }
            }
        } else {
            return Ok(false);
        }
        
        Ok(true)
    }

    /// reconcileElectionStep steps through a step in an election.
    /// A step looks at the current state of Lease and LeaseCandidates and takes one of the following action
    /// - do nothing (because leader is already optimal or still waiting for an event)
    /// - request ack from candidates (update LeaseCandidate PingTime)
    /// - finds the most optimal candidate and elect (update the Lease object)
    /// Instead of keeping a map and lock on election, the state is
    /// calculated every time by looking at the lease, and set of available candidates.
    /// PingTime + electionDuration > time.Now: We just asked all candidates to ack and are still waiting for response
    /// PingTime + electionDuration < time.Now: Candidate has not responded within the appropriate PingTime. Continue the election.
    /// RenewTime + 5 seconds > time.Now: All candidates acked in the last 5 seconds, continue the election.
    async fn reconcile_election_step(
        &self,
        lease_name: &str,
        namespace: &str,
    ) -> Result<Duration, ControllerError> {
        let candidates = self.list_admissable_candidates(lease_name, namespace).await?;
        
        if candidates.is_empty() {
            return Ok(NO_REQUEUE);
        }
        
        tracing::trace!(
            "Reconciling election for {}/{}, candidates: {}",
            namespace,
            lease_name,
            candidates.len()
        );

        // Check if an election is really needed by looking at the current lease and candidates
        let need_election = self.election_needed(&candidates, lease_name, namespace).await?;
        if !need_election {
            return Ok(DEFAULT_REQUEUE_INTERVAL);
        }

        let now = Instant::now();
        let mut can_vote_yet = true;
        
        for candidate in &candidates {
            if let (Some(ping_time), Some(renew_time)) = (&candidate.spec.ping_time, &candidate.spec.renew_time) {
                let ping_instant = time_to_instant(&ping_time.0);
                let renew_instant = time_to_instant(&renew_time.0);
                
                if ping_instant + ELECTION_DURATION > now && renew_instant < ping_instant {
                    // continue waiting for the election to timeout
                    can_vote_yet = false;
                    continue;
                }
            }
            
            if let Some(renew_time) = &candidate.spec.renew_time {
                let renew_instant = time_to_instant(&renew_time.0);
                if renew_instant + ELECTION_DURATION > now {
                    continue;
                }
            }

            if candidate.spec.ping_time.is_none() ||
                (candidate.spec.ping_time.as_ref().map(|pt| {
                    let ping_instant = time_to_instant(&pt.0);
                    ping_instant + ELECTION_DURATION < now &&
                        candidate.spec.renew_time.as_ref().map_or(false, |rt| {
                            time_to_instant(&rt.0) >= ping_instant
                        })
                }).unwrap_or(false))
            {
                let mut clone = candidate.clone();
                clone.spec.ping_time = Some(MicroTime(chrono::Utc::now()));
                
                let patch = Patch::Merge(&clone);
                self.lease_candidate_api
                    .patch(&candidate.metadata.name.as_ref().unwrap(), &PatchParams::default(), &patch)
                    .await?;
                can_vote_yet = false;
            }
        }
        
        if !can_vote_yet {
            return Ok(DEFAULT_REQUEUE_INTERVAL);
        }

        // election is ongoing as long as unexpired PingTimes exist
        for candidate in &candidates {
            if candidate.spec.ping_time.is_none() {
                continue; // shouldn't be the case after the above
            }

            if let Some(renew_time) = &candidate.spec.renew_time {
                if let Some(ping_time) = &candidate.spec.ping_time {
                    if time_to_instant(&renew_time.0) >= time_to_instant(&ping_time.0) {
                        continue; // this has renewed already
                    }
                }
            }

            // If a candidate has a PingTime within the election duration, they have not acked
            // and we should wait until we receive their response
            if let Some(ping_time) = &candidate.spec.ping_time {
                if time_to_instant(&ping_time.0) + ELECTION_DURATION > now {
                    // continue waiting for the election to timeout
                    return Ok(NO_REQUEUE);
                }
            }
        }

        let mut acked_candidates = Vec::new();
        for candidate in &candidates {
            if let Some(renew_time) = &candidate.spec.renew_time {
                if time_to_instant(&renew_time.0) + ELECTION_DURATION > now {
                    acked_candidates.push(candidate.clone());
                }
            }
        }
        
        if acked_candidates.is_empty() {
            return Err(ControllerError::NoAvailableCandidates);
        }

        let strategy = pick_best_strategy(&acked_candidates)?;

        let mut leader_lease = Lease {
            metadata: ObjectMeta {
                namespace: Some(namespace.to_string()),
                name: Some(lease_name.to_string()),
                ..Default::default()
            },
            spec: Some(LeaseSpec {
                strategy: Some(strategy.clone()),
                lease_duration_seconds: Some(DEFAULT_LEASE_DURATION_SECONDS),
                renew_time: Some(MicroTime(chrono::Utc::now())),
                ..Default::default()
            }),
        };

        match strategy.as_str() {
            "OldestEmulationVersion" => {
                let electee = pick_best_leader_oldest_emulation_version(&acked_candidates)
                    .ok_or(ControllerError::NoSuitableElectee)?;
                if let Some(spec) = &mut leader_lease.spec {
                    spec.holder_identity = electee.metadata.name.clone();
                }
            }
            _ => {
                // do not set the holder identity, but leave it to some other controller. But fall
                // through to create the lease (without holder).
                tracing::debug!(
                    "Election for strategy {:?} is not handled by {}",
                    strategy,
                    CONTROLLER_NAME
                );
            }
        }

        // create the leader election lease
        match self.lease_api.create(&PostParams::default(), &leader_lease).await {
            Ok(_) => {
                if let Some(spec) = &leader_lease.spec {
                    if let Some(holder) = &spec.holder_identity {
                        info!("Created lease {}/{} for {:?}", namespace, lease_name, holder);
                    } else {
                        info!("Created lease {}/{} without leader", namespace, lease_name);
                    }
                }
                return Ok(DEFAULT_REQUEUE_INTERVAL);
            }
            Err(kube::Error::Api(err)) if err.code == 409 => {
                // Already exists, continue to update
            }
            Err(e) => {
                return Err(ControllerError::KubeError(e));
            }
        }

        // Get existing lease
        let existing = self.lease_api.get(lease_name).await?;
        let orig = existing.clone();

        let mut updated = existing.clone();
        let is_expired = is_lease_expired(&existing);
        
        let no_holder_identity = leader_lease.spec.as_ref()
            .and_then(|s| s.holder_identity.as_ref())
            .is_some() && existing.spec.as_ref()
            .and_then(|s| s.holder_identity.as_ref())
            .map_or(true, |h| h.is_empty());
            
        let expired_and_new_holder = is_expired && 
            leader_lease.spec.as_ref().and_then(|s| s.holder_identity.as_ref()).is_some() &&
            existing.spec.as_ref().and_then(|s| s.holder_identity.as_ref()) != 
            leader_lease.spec.as_ref().and_then(|s| s.holder_identity.as_ref());
            
        let strategy_changed = existing.spec.as_ref()
            .and_then(|s| s.strategy.as_ref())
            .map_or(true, |s| s != &strategy);
            
        let different_holder = leader_lease.spec.as_ref()
            .and_then(|s| s.holder_identity.as_ref())
            .is_some() && 
            leader_lease.spec.as_ref().and_then(|s| s.holder_identity.as_ref()) !=
            existing.spec.as_ref().and_then(|s| s.holder_identity.as_ref());

        // Update lease
        if strategy_changed {
            info!("Lease {}/{} strategy changed to {:?}", namespace, lease_name, strategy);
            if let Some(spec) = &mut updated.spec {
                spec.strategy = Some(strategy.clone());
            }
        }
        
        if no_holder_identity || expired_and_new_holder {
            if let Some(spec) = &mut updated.spec {
                if no_holder_identity {
                    info!(
                        "Lease {}/{} had no holder, setting holder to {:?}",
                        namespace,
                        lease_name,
                        leader_lease.spec.as_ref().and_then(|s| s.holder_identity.as_ref())
                    );
                } else {
                    info!(
                        "Lease {}/{} expired, resetting it and setting holder to {:?}",
                        namespace,
                        lease_name,
                        leader_lease.spec.as_ref().and_then(|s| s.holder_identity.as_ref())
                    );
                }

                spec.preferred_holder = None;
                spec.holder_identity = leader_lease.spec.as_ref()
                    .and_then(|s| s.holder_identity.clone());
                spec.renew_time = Some(MicroTime(chrono::Utc::now()));
                spec.lease_duration_seconds = Some(DEFAULT_LEASE_DURATION_SECONDS);
                spec.acquire_time = None;
            }
        } else if different_holder {
            if let Some(spec) = &mut updated.spec {
                info!(
                    "Lease {}/{} holder changed from {:?} to {:?}",
                    namespace,
                    lease_name,
                    spec.holder_identity,
                    leader_lease.spec.as_ref().and_then(|s| s.holder_identity.as_ref())
                );
                spec.preferred_holder = leader_lease.spec.as_ref()
                    .and_then(|s| s.holder_identity.clone());
            }
        }

        if updated == orig {
            if let Some(spec) = &existing.spec {
                if let Some(holder) = &spec.holder_identity {
                    tracing::debug!(
                        "Lease {} is managed by a third party strategy",
                        holder
                    );
                } else {
                    tracing::debug!(
                        "Lease {}/{} already has the most optimal leader",
                        namespace,
                        lease_name
                    );
                }
            }
            // We need to requeue to ensure that we are aware of an expired lease
            return Ok(DEFAULT_REQUEUE_INTERVAL);
        }

        self.lease_api
            .replace(lease_name, &PostParams::default(), &updated)
            .await?;

        Ok(DEFAULT_REQUEUE_INTERVAL)
    }

    async fn list_admissable_candidates(
        &self,
        lease_name: &str,
        namespace: &str,
    ) -> Result<Vec<LeaseCandidate>, ControllerError> {
        let leases = self.lease_candidate_api.list(&ListParams::default()).await?;
        
        let mut results = Vec::new();
        for l in leases.items {
            if l.spec.lease_name != lease_name {
                continue;
            }
            if !is_lease_candidate_expired(&l) {
                results.push(l);
            } else {
                info!("LeaseCandidate {} is expired", l.metadata.name.as_ref().unwrap_or(&"unknown".to_string()));
            }
        }
        
        Ok(results)
    }
}

impl Clone for Controller {
    fn clone(&self) -> Self {
        Self {
            lease_api: self.lease_api.clone(),
            lease_candidate_api: self.lease_candidate_api.clone(),
            client: self.client.clone(),
        }
    }
}

fn pick_best_leader_oldest_emulation_version(candidates: &[LeaseCandidate]) -> Option<LeaseCandidate> {
    let mut electee: Option<LeaseCandidate> = None;
    
    for c in candidates {
        if !valid_lease_candidate_for_oldest_emulation_version(c) {
            continue;
        }
        if electee.is_none() || compare(electee.as_ref().unwrap(), c) > 0 {
            electee = Some(c.clone());
        }
    }
    
    electee
}

fn pick_best_strategy(candidates: &[LeaseCandidate]) -> Result<String, ControllerError> {
    if candidates.is_empty() {
        return Err(ControllerError::NoCandidates);
    }
    
    let candidate_name = candidates[0].metadata.name.as_ref().unwrap();
    let mut strategy = candidates[0].spec.strategy.clone();
    let mut highest_bv = get_binary_version_or_zero(&candidates[0]);

    for c in &candidates[1..] {
        let bin_version = get_binary_version_or_zero(c);
        let result = highest_bv.cmp(&bin_version);
        
        match result {
            std::cmp::Ordering::Less => {
                strategy = c.spec.strategy.clone();
                highest_bv = bin_version;
            }
            std::cmp::Ordering::Equal => {
                if c.spec.strategy != strategy {
                    return Err(ControllerError::ConflictingStrategies(
                        strategy,
                        c.spec.strategy.clone(),
                    ));
                }
            }
            _ => {}
        }
    }
    
    Ok(strategy)
}

fn valid_lease_candidate_for_oldest_emulation_version(l: &LeaseCandidate) -> bool {
    Version::parse(&l.spec.emulation_version).is_ok() &&
        Version::parse(&l.spec.binary_version).is_ok()
}

fn get_emulation_version_or_zero(l: &LeaseCandidate) -> Version {
    Version::parse(&l.spec.emulation_version).unwrap_or(Version::new(0, 0, 0))
}

fn get_binary_version_or_zero(l: &LeaseCandidate) -> Version {
    Version::parse(&l.spec.binary_version).unwrap_or(Version::new(0, 0, 0))
}

/// -1: lhs better, 1: rhs better
fn compare(lhs: &LeaseCandidate, rhs: &LeaseCandidate) -> i32 {
    let l = get_emulation_version_or_zero(lhs);
    let r = get_emulation_version_or_zero(rhs);
    
    match l.cmp(&r) {
        std::cmp::Ordering::Equal => {
            let l = get_binary_version_or_zero(lhs);
            let r = get_binary_version_or_zero(rhs);
            
            match l.cmp(&r) {
                std::cmp::Ordering::Equal => {
                    if lhs.metadata.creation_timestamp > rhs.metadata.creation_timestamp {
                        1
                    } else {
                        -1
                    }
                }
                std::cmp::Ordering::Less => -1,
                std::cmp::Ordering::Greater => 1,
            }
        }
        std::cmp::Ordering::Less => -1,
        std::cmp::Ordering::Greater => 1,
    }
}

fn is_lease_expired(lease: &Lease) -> bool {
    let now = chrono::Utc::now();
    
    lease.spec.as_ref().map_or(true, |spec| {
        spec.renew_time.is_none() ||
            spec.lease_duration_seconds.is_none() ||
            spec.renew_time.as_ref().map_or(true, |rt| {
                let duration = Duration::from_secs(spec.lease_duration_seconds.unwrap() as u64);
                rt.0 + chrono::Duration::from_std(duration).unwrap() < now
            })
    })
}

fn is_lease_candidate_expired(lease: &LeaseCandidate) -> bool {
    let now = chrono::Utc::now();
    
    lease.spec.renew_time.is_none() ||
        lease.spec.renew_time.as_ref().map_or(true, |rt| {
            rt.0 + chrono::Duration::from_std(LEASE_CANDIDATE_VALID_DURATION).unwrap() < now
        })
}

fn time_to_instant(time: &chrono::DateTime<chrono::Utc>) -> Instant {
    // This is a simplified conversion - in production you'd need proper time handling
    let now = chrono::Utc::now();
    let diff = time.signed_duration_since(now);
    
    if diff.num_milliseconds() > 0 {
        Instant::now() + Duration::from_millis(diff.num_milliseconds() as u64)
    } else {
        Instant::now() - Duration::from_millis((-diff.num_milliseconds()) as u64)
    }
}

